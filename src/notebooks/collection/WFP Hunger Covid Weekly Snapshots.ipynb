{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WFP Hunger Covid Weekly Snapshots\n",
    "\n",
    "Running this notebooks collects the national and urban weekly snapshots about Hunger and Covid, created by the World Food Programm (WFP). They are all downloaded (see parameter `output_folder_path`) and a metadata csv is created (see parameter `output_metadata_file_path`).\n",
    "\n",
    "#### How to maintain this code?\n",
    "New countries or urban areas may be added to the [Snapshots listing pdf](https://static.hungermapdata.org/hungermap/reports/hunger_covid_weekly_snapshot.pdf\") over time. If so, they will appear in the last cell output. To include them in the collection, edit the \"hyperlinks mapping\" csv file (see parameter `countries_code_path`) to add them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import tqdm\n",
    "\n",
    "# data processing\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# conf\n",
    "import sys\n",
    "from path_manipulation import get_to_root\n",
    "\n",
    "sys.path[0] = get_to_root(3,sys.path[0])\n",
    "from config.config import config\n",
    "sources = config.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_from_url(url, output_path=None):\n",
    "    \"\"\"Download a pdf from a given url.\n",
    "    :param url: web url from where to download the pdf\n",
    "    :param output_path: a pathlib.Path object to which to write the downloaded pdf\n",
    "    \"\"\"\n",
    "    # create the folder if not existing\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # download\n",
    "    urllib.request.urlretrieve(url, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_pdf_urls_from_countries_snapshots_overview(\n",
    "    overview_path=sources.wpf_hunger_covid_snapshots_overview_path,\n",
    "):\n",
    "    # Returns links_metadata, a list of (country_name, url) pairs\n",
    "    # Note: this snippet is adapted (+ corrected) from : https://github.com/jsvine/pdfplumber/issues/151\n",
    "\n",
    "    def try_resolve(x):\n",
    "        # Some of the PDF's properties are encoded as strings, while others are encoded as PDFObjects.\n",
    "        # Here, we resolve them all to strings, rather than guess\n",
    "        try:\n",
    "            return x.resolve()\n",
    "        except:\n",
    "            return x\n",
    "\n",
    "    def rect_to_bbox(rect, page):\n",
    "        # Just to transform the PDF spec's version of bounding boxes\n",
    "        # into more common coordinates\n",
    "        return [\n",
    "            rect[0],\n",
    "            float(page.height) - rect[3],\n",
    "            rect[2],\n",
    "            float(page.height) - rect[1],\n",
    "        ]\n",
    "\n",
    "    def generate_links(page):\n",
    "        for anno in page.page_obj.annots:\n",
    "            anno = anno.resolve()\n",
    "            if anno.get(\"Subtype\").name != \"Link\":\n",
    "                continue\n",
    "\n",
    "            yield {\n",
    "                \"url\": try_resolve(anno[\"A\"])[\"URI\"].decode(\"utf-8\"),\n",
    "                \"bbox\": rect_to_bbox(try_resolve(anno[\"Rect\"]), page),\n",
    "            }\n",
    "\n",
    "    with pdfplumber.open(overview_path) as pdf:\n",
    "        my_page = pdf.pages[2]\n",
    "        links = list(generate_links(my_page))\n",
    "        links_metadata = []\n",
    "        for link in links:\n",
    "\n",
    "            url = link[\"url\"]\n",
    "            bbox = link[\"bbox\"]\n",
    "\n",
    "            subpage = my_page.crop(bbox, relative=False)\n",
    "            text = subpage.extract_text(x_tolerance=3, y_tolerance=3)\n",
    "\n",
    "            if \"EN\" in text:\n",
    "                # if there are two languages, look on the left to find the country\n",
    "                bbox[0] = bbox[0] - int(140 / 3)\n",
    "                subpage = my_page.crop(bbox, relative=False)\n",
    "                text = subpage.extract_text(x_tolerance=3, y_tolerance=3)\n",
    "                # remove \": EN\" from e.g. \"Honduras EN:\" text\n",
    "                text = text.replace(\": EN\", \"\")\n",
    "            text = text.replace(\"o \", \"\").replace(\"− \", \"\")\n",
    "            link_metadata = (text, url)\n",
    "            links_metadata.append(link_metadata)\n",
    "\n",
    "    return links_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add to utils\n",
    "def get_current_date_as_str():\n",
    "    date_str = datetime.now().strftime(\"%Y%d%m\")\n",
    "    return date_str\n",
    "\n",
    "\n",
    "def get_countries_pdf_from_countries_url(\n",
    "    links_metadata,\n",
    "    countries_code_path=sources.wfp_hunger_covid_snapshots_hyperlinks_mapping_path,\n",
    "    output_folder_path=sources.wfp_hunger_covid_weekly_snapshots_folder_path,\n",
    "    output_metadata_file_path=sources.wfp_hunger_covid_weekly_snapshots_metadata_path,\n",
    "):\n",
    "    # This maps names in the snapshots overview to countries iso code and a formated filename,\n",
    "    # then download all files in a folder. If the folder already exists, it is deleted and will\n",
    "    # be re-created with the new files.\n",
    "    # Additionaly, a csv is constructed with the snapshots metadata:\n",
    "    # country_iso, hyperlink_name, hyperlink_url, formated_name, filename, date of update\n",
    "    try:\n",
    "        shutil.rmtree(output_folder_path)\n",
    "    except:\n",
    "        # folder already deleted\n",
    "        pass\n",
    "    # Get the name:country_iso and name:formated_name mappings\n",
    "    # using a custom mapping adapted to WFP snapshots' names.\n",
    "    countries_code = pd.read_csv(countries_code_path, sep=\";\")\n",
    "    countries_code = countries_code[countries_code.include_this_url == 1].drop(\n",
    "        columns=\"include_this_url\"\n",
    "    )\n",
    "    countries_code = countries_code.set_index(\"name_in_weekly_snapshot\").to_dict()\n",
    "    hyperkink_names_known_to_be_ignored = [\n",
    "        \"CLICK TO DOWNLOAD ALL NATIONAL SNAPSHOTS \",\n",
    "        \" ES\",\n",
    "        \"of the Congo\",\n",
    "        \" ES\",\n",
    "        \" ES\",\n",
    "        \" FR\",\n",
    "        \" ES\",\n",
    "        \"Republic\",\n",
    "        \"DOWNLOAD TODAY’S DAILY SNAPSHOT \",\n",
    "        \"CLICK TO DOWNLOAD ALL URBAN SNAPSHOTS\",\n",
    "        \"@WFPVAM|\",\n",
    "        \" @mobileVAM\",\n",
    "        \"mvam.org\",\n",
    "    ]\n",
    "\n",
    "    full_metadatas = []\n",
    "    print(\n",
    "        \"Ignored Hyperlinks will be listed below (if any). \\n N.B.: Consider inclusion if this is a new country/zone considered by the WFP.\"\n",
    "    )\n",
    "    for hyperlink_name, hyperlink_url in tqdm.tqdm(links_metadata):\n",
    "        # get the country iso and formated name from the hyperlink's text.\n",
    "        date_str = get_current_date_as_str()\n",
    "        try:\n",
    "            country_iso = countries_code[\"iso_3166_1_code\"][hyperlink_name]\n",
    "            formated_name = countries_code[\"formated_name\"][hyperlink_name]\n",
    "            output_filename = \"snapshot_{}_iso_{}_country_{}.pdf\".format(\n",
    "                date_str, country_iso, formated_name\n",
    "            )\n",
    "            output_path = output_folder_path / output_filename\n",
    "            get_pdf_from_url(hyperlink_url, output_path=output_path)\n",
    "            full_metadatas.append(\n",
    "                [\n",
    "                    country_iso,\n",
    "                    hyperlink_name,\n",
    "                    hyperlink_url,\n",
    "                    formated_name,\n",
    "                    str(output_path),\n",
    "                    date_str,\n",
    "                ]\n",
    "            )\n",
    "        except e:\n",
    "\n",
    "            if hyperlink_name not in hyperkink_names_known_to_be_ignored:\n",
    "                print(\"-     '{}'\".format(hyperlink_name))\n",
    "                print(\"Associated error:\", e)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        full_metadatas,\n",
    "        columns=[\n",
    "            \"country_iso\",\n",
    "            \"hyperlink_name\",\n",
    "            \"hyperlink_url\",\n",
    "            \"formated_name\",\n",
    "            \"output_path\",\n",
    "            \"date_of_update_YYYYMMDD\",\n",
    "        ],\n",
    "    )\n",
    "    df.to_csv(output_metadata_file_path, sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get snapshots overview and scrap listing for all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                              | 0/65 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored Hyperlinks will be listed below (if any). \n",
      " N.B.: Consider inclusion if this is a new country/zone considered by the WFP.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [00:48<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "overview_url = sources.wfp_hunger_covid_snapshots_overview_url\n",
    "overview_path = sources.wpf_hunger_covid_snapshots_overview_path\n",
    "# download the pdf, save it\n",
    "get_pdf_from_url(overview_url, output_path=overview_path)\n",
    "# Get the url from the hyperlinks in the snapshot overview\n",
    "links_metadata = scrap_pdf_urls_from_countries_snapshots_overview(\n",
    "    overview_path=overview_path\n",
    ")\n",
    "# map each location to a country's Iso code and download the pdf.\n",
    "get_countries_pdf_from_countries_url(links_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dev"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "dev"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                              | 0/65 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored Hyperlinks will be listed below (if any). \n",
      " N.B.: Consider inclusion if this is a new country/zone considered by the WFP.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c69411cf361e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mcountry_iso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcountries_code\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"iso_3166_1_code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhyperlink_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mformated_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcountries_code\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"formated_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhyperlink_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CLICK TO DOWNLOAD ALL NATIONAL SNAPSHOTS '",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c69411cf361e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m#             ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;31m#         )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhyperlink_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhyperkink_names_known_to_be_ignored\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "# this is for development purpose / debogage\n",
    "\n",
    "links_metadata\n",
    "countries_code_path=sources.wfp_hunger_covid_snapshots_hyperlinks_mapping_path\n",
    "output_folder_path=sources.wfp_hunger_covid_weekly_snapshots_folder_path\n",
    "output_metadata_file_path=sources.wfp_hunger_covid_weekly_snapshots_metadata_path\n",
    "try:\n",
    "    shutil.rmtree(output_folder_path)\n",
    "except:\n",
    "    # folder already deleted\n",
    "    pass\n",
    "# Get the name:country_iso and name:formated_name mappings\n",
    "# using a custom mapping adapted to WFP snapshots' names.\n",
    "countries_code = pd.read_csv(countries_code_path, sep=\";\")\n",
    "countries_code = countries_code[countries_code.include_this_url == 1].drop(\n",
    "    columns=\"include_this_url\"\n",
    ")\n",
    "countries_code = countries_code.set_index(\"name_in_weekly_snapshot\").to_dict()\n",
    "hyperkink_names_known_to_be_ignored = [\n",
    "    \"CLICK TO DOWNLOAD ALL NATIONAL SNAPSHOTS \",\n",
    "    \" ES\",\n",
    "    \"of the Congo\",\n",
    "    \" ES\",\n",
    "    \" ES\",\n",
    "    \" FR\",\n",
    "    \" ES\",\n",
    "    \"Republic\",\n",
    "    \"DOWNLOAD TODAY’S DAILY SNAPSHOT \",\n",
    "    \"CLICK TO DOWNLOAD ALL URBAN SNAPSHOTS\",\n",
    "    \"@WFPVAM|\",\n",
    "    \" @mobileVAM\",\n",
    "    \"mvam.org\",\n",
    "]\n",
    "\n",
    "full_metadatas = []\n",
    "print(\n",
    "    \"Ignored Hyperlinks will be listed below (if any). \\n N.B.: Consider inclusion if this is a new country/zone considered by the WFP.\"\n",
    ")\n",
    "for hyperlink_name, hyperlink_url in tqdm.tqdm(links_metadata):\n",
    "    # get the country iso and formated name from the hyperlink's text.\n",
    "    date_str = get_current_date_as_str()\n",
    "    \n",
    "    try:\n",
    "        country_iso = countries_code[\"iso_3166_1_code\"][hyperlink_name]\n",
    "        formated_name = countries_code[\"formated_name\"][hyperlink_name]\n",
    "        output_filename = \"snapshot_{}_iso_{}_country_{}.pdf\".format(\n",
    "            date_str, country_iso, formated_name\n",
    "        )\n",
    "        print(output_filename)\n",
    "        output_path = output_folder_path / output_filename\n",
    "        get_pdf_from_url(hyperlink_url, output_path=output_path)\n",
    "        print(str(output_path))\n",
    "#         full_metadatas.append(\n",
    "#             [\n",
    "#                 country_iso,\n",
    "#                 hyperlink_name,\n",
    "#                 hyperlink_url,\n",
    "#                 formated_name,\n",
    "#                 str(output_path),\n",
    "#                 date_str,\n",
    "#             ]\n",
    "#         )\n",
    "    except e:\n",
    "        print(e)\n",
    "        if hyperlink_name not in hyperkink_names_known_to_be_ignored:\n",
    "            print(\"-     '{}'\".format(hyperlink_name))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mvam.org'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperlink_name"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
